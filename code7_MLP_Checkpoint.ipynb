{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.398695\n",
      "model saved to ./save\\ckpt-1\n",
      "batch 1: loss 2.378308\n",
      "model saved to ./save\\ckpt-2\n",
      "batch 2: loss 2.213981\n",
      "model saved to ./save\\ckpt-3\n",
      "batch 3: loss 2.139094\n",
      "model saved to ./save\\ckpt-4\n",
      "batch 4: loss 2.036330\n",
      "model saved to ./save\\ckpt-5\n",
      "batch 5: loss 2.022789\n",
      "model saved to ./save\\ckpt-6\n",
      "batch 6: loss 1.992425\n",
      "model saved to ./save\\ckpt-7\n",
      "batch 7: loss 1.841374\n",
      "model saved to ./save\\ckpt-8\n",
      "batch 8: loss 1.950590\n",
      "model saved to ./save\\ckpt-9\n",
      "batch 9: loss 1.888216\n",
      "model saved to ./save\\ckpt-10\n",
      "batch 10: loss 1.758012\n",
      "model saved to ./save\\ckpt-11\n",
      "batch 11: loss 1.589187\n",
      "model saved to ./save\\ckpt-12\n",
      "batch 12: loss 1.567833\n",
      "model saved to ./save\\ckpt-13\n",
      "batch 13: loss 1.533204\n",
      "model saved to ./save\\ckpt-14\n",
      "batch 14: loss 1.397109\n",
      "model saved to ./save\\ckpt-15\n",
      "batch 15: loss 1.529853\n",
      "model saved to ./save\\ckpt-16\n",
      "batch 16: loss 1.459800\n",
      "model saved to ./save\\ckpt-17\n",
      "batch 17: loss 1.483278\n",
      "model saved to ./save\\ckpt-18\n",
      "batch 18: loss 1.449621\n",
      "model saved to ./save\\ckpt-19\n",
      "batch 19: loss 1.353379\n",
      "model saved to ./save\\ckpt-20\n",
      "batch 20: loss 1.182459\n",
      "model saved to ./save\\ckpt-21\n",
      "batch 21: loss 1.326990\n",
      "model saved to ./save\\ckpt-22\n",
      "batch 22: loss 1.145034\n",
      "model saved to ./save\\ckpt-23\n",
      "batch 23: loss 1.049387\n",
      "model saved to ./save\\ckpt-24\n",
      "batch 24: loss 1.021447\n",
      "model saved to ./save\\ckpt-25\n",
      "batch 25: loss 1.025935\n",
      "model saved to ./save\\ckpt-26\n",
      "batch 26: loss 0.850563\n",
      "model saved to ./save\\ckpt-27\n",
      "batch 27: loss 0.996184\n",
      "model saved to ./save\\ckpt-28\n",
      "batch 28: loss 1.030242\n",
      "model saved to ./save\\ckpt-29\n",
      "batch 29: loss 0.945938\n",
      "model saved to ./save\\ckpt-30\n",
      "batch 30: loss 1.083658\n",
      "model saved to ./save\\ckpt-31\n",
      "batch 31: loss 0.805820\n",
      "model saved to ./save\\ckpt-32\n",
      "batch 32: loss 1.017290\n",
      "model saved to ./save\\ckpt-33\n",
      "batch 33: loss 0.641492\n",
      "model saved to ./save\\ckpt-34\n",
      "batch 34: loss 0.721208\n",
      "model saved to ./save\\ckpt-35\n",
      "batch 35: loss 0.973117\n",
      "model saved to ./save\\ckpt-36\n",
      "batch 36: loss 0.817867\n",
      "model saved to ./save\\ckpt-37\n",
      "batch 37: loss 0.779149\n",
      "model saved to ./save\\ckpt-38\n",
      "batch 38: loss 0.994412\n",
      "model saved to ./save\\ckpt-39\n",
      "batch 39: loss 0.909220\n",
      "model saved to ./save\\ckpt-40\n",
      "batch 40: loss 0.662413\n",
      "model saved to ./save\\ckpt-41\n",
      "batch 41: loss 0.869590\n",
      "model saved to ./save\\ckpt-42\n",
      "batch 42: loss 0.643076\n",
      "model saved to ./save\\ckpt-43\n",
      "batch 43: loss 0.591606\n",
      "model saved to ./save\\ckpt-44\n",
      "batch 44: loss 0.722453\n",
      "model saved to ./save\\ckpt-45\n",
      "batch 45: loss 0.923980\n",
      "model saved to ./save\\ckpt-46\n",
      "batch 46: loss 0.486323\n",
      "model saved to ./save\\ckpt-47\n",
      "batch 47: loss 0.599103\n",
      "model saved to ./save\\ckpt-48\n",
      "batch 48: loss 0.552839\n",
      "model saved to ./save\\ckpt-49\n",
      "batch 49: loss 0.708987\n",
      "model saved to ./save\\ckpt-50\n",
      "batch 50: loss 0.622436\n",
      "model saved to ./save\\ckpt-51\n",
      "batch 51: loss 0.613451\n",
      "model saved to ./save\\ckpt-52\n",
      "batch 52: loss 0.543326\n",
      "model saved to ./save\\ckpt-53\n",
      "batch 53: loss 0.619560\n",
      "model saved to ./save\\ckpt-54\n",
      "batch 54: loss 0.666405\n",
      "model saved to ./save\\ckpt-55\n",
      "batch 55: loss 0.735301\n",
      "model saved to ./save\\ckpt-56\n",
      "batch 56: loss 0.680968\n",
      "model saved to ./save\\ckpt-57\n",
      "batch 57: loss 0.594975\n",
      "model saved to ./save\\ckpt-58\n",
      "batch 58: loss 0.536590\n",
      "model saved to ./save\\ckpt-59\n",
      "batch 59: loss 0.567625\n",
      "model saved to ./save\\ckpt-60\n",
      "batch 60: loss 0.607826\n",
      "model saved to ./save\\ckpt-61\n",
      "batch 61: loss 0.543904\n",
      "model saved to ./save\\ckpt-62\n",
      "batch 62: loss 0.498617\n",
      "model saved to ./save\\ckpt-63\n",
      "batch 63: loss 0.539854\n",
      "model saved to ./save\\ckpt-64\n",
      "batch 64: loss 0.490167\n",
      "model saved to ./save\\ckpt-65\n",
      "batch 65: loss 0.485635\n",
      "model saved to ./save\\ckpt-66\n",
      "batch 66: loss 0.391285\n",
      "model saved to ./save\\ckpt-67\n",
      "batch 67: loss 0.479965\n",
      "model saved to ./save\\ckpt-68\n",
      "batch 68: loss 0.477599\n",
      "model saved to ./save\\ckpt-69\n",
      "batch 69: loss 0.554438\n",
      "model saved to ./save\\ckpt-70\n",
      "batch 70: loss 0.452378\n",
      "model saved to ./save\\ckpt-71\n",
      "batch 71: loss 0.460950\n",
      "model saved to ./save\\ckpt-72\n",
      "batch 72: loss 0.387339\n",
      "model saved to ./save\\ckpt-73\n",
      "batch 73: loss 0.612081\n",
      "model saved to ./save\\ckpt-74\n",
      "batch 74: loss 0.560972\n",
      "model saved to ./save\\ckpt-75\n",
      "batch 75: loss 0.472927\n",
      "model saved to ./save\\ckpt-76\n",
      "batch 76: loss 0.433416\n",
      "model saved to ./save\\ckpt-77\n",
      "batch 77: loss 0.677270\n",
      "model saved to ./save\\ckpt-78\n",
      "batch 78: loss 0.554391\n",
      "model saved to ./save\\ckpt-79\n",
      "batch 79: loss 0.439683\n",
      "model saved to ./save\\ckpt-80\n",
      "batch 80: loss 0.527752\n",
      "model saved to ./save\\ckpt-81\n",
      "batch 81: loss 0.418399\n",
      "model saved to ./save\\ckpt-82\n",
      "batch 82: loss 0.395372\n",
      "model saved to ./save\\ckpt-83\n",
      "batch 83: loss 0.473681\n",
      "model saved to ./save\\ckpt-84\n",
      "batch 84: loss 0.665173\n",
      "model saved to ./save\\ckpt-85\n",
      "batch 85: loss 0.595474\n",
      "model saved to ./save\\ckpt-86\n",
      "batch 86: loss 0.482671\n",
      "model saved to ./save\\ckpt-87\n",
      "batch 87: loss 0.404593\n",
      "model saved to ./save\\ckpt-88\n",
      "batch 88: loss 0.447197\n",
      "model saved to ./save\\ckpt-89\n",
      "batch 89: loss 0.557101\n",
      "model saved to ./save\\ckpt-90\n",
      "batch 90: loss 0.744751\n",
      "model saved to ./save\\ckpt-91\n",
      "batch 91: loss 0.483241\n",
      "model saved to ./save\\ckpt-92\n",
      "batch 92: loss 0.500440\n",
      "model saved to ./save\\ckpt-93\n",
      "batch 93: loss 0.329923\n",
      "model saved to ./save\\ckpt-94\n",
      "batch 94: loss 0.498903\n",
      "model saved to ./save\\ckpt-95\n",
      "batch 95: loss 0.501249\n",
      "model saved to ./save\\ckpt-96\n",
      "batch 96: loss 0.594513\n",
      "model saved to ./save\\ckpt-97\n",
      "batch 97: loss 0.572962\n",
      "model saved to ./save\\ckpt-98\n",
      "batch 98: loss 0.480774\n",
      "model saved to ./save\\ckpt-99\n",
      "batch 99: loss 0.445274\n",
      "model saved to ./save\\ckpt-100\n",
      "batch 100: loss 0.369240\n",
      "model saved to ./save\\ckpt-101\n",
      "batch 101: loss 0.338594\n",
      "model saved to ./save\\ckpt-102\n",
      "batch 102: loss 0.361621\n",
      "model saved to ./save\\ckpt-103\n",
      "batch 103: loss 0.416681\n",
      "model saved to ./save\\ckpt-104\n",
      "batch 104: loss 0.530676\n",
      "model saved to ./save\\ckpt-105\n",
      "batch 105: loss 0.261504\n",
      "model saved to ./save\\ckpt-106\n",
      "batch 106: loss 0.356906\n",
      "model saved to ./save\\ckpt-107\n",
      "batch 107: loss 0.376252\n",
      "model saved to ./save\\ckpt-108\n",
      "batch 108: loss 0.415951\n",
      "model saved to ./save\\ckpt-109\n",
      "batch 109: loss 0.359256\n",
      "model saved to ./save\\ckpt-110\n",
      "batch 110: loss 0.465459\n",
      "model saved to ./save\\ckpt-111\n",
      "batch 111: loss 0.402539\n",
      "model saved to ./save\\ckpt-112\n",
      "batch 112: loss 0.742508\n",
      "model saved to ./save\\ckpt-113\n",
      "batch 113: loss 0.294256\n",
      "model saved to ./save\\ckpt-114\n",
      "batch 114: loss 0.346270\n",
      "model saved to ./save\\ckpt-115\n",
      "batch 115: loss 0.291681\n",
      "model saved to ./save\\ckpt-116\n",
      "batch 116: loss 0.315126\n",
      "model saved to ./save\\ckpt-117\n",
      "batch 117: loss 0.542845\n",
      "model saved to ./save\\ckpt-118\n",
      "batch 118: loss 0.415556\n",
      "model saved to ./save\\ckpt-119\n",
      "batch 119: loss 0.633570\n",
      "model saved to ./save\\ckpt-120\n",
      "batch 120: loss 0.477262\n",
      "model saved to ./save\\ckpt-121\n",
      "batch 121: loss 0.468697\n",
      "model saved to ./save\\ckpt-122\n",
      "batch 122: loss 0.271076\n",
      "model saved to ./save\\ckpt-123\n",
      "batch 123: loss 0.668440\n",
      "model saved to ./save\\ckpt-124\n",
      "batch 124: loss 0.333050\n",
      "model saved to ./save\\ckpt-125\n",
      "batch 125: loss 0.271761\n",
      "model saved to ./save\\ckpt-126\n",
      "batch 126: loss 0.514943\n",
      "model saved to ./save\\ckpt-127\n",
      "batch 127: loss 0.552212\n",
      "model saved to ./save\\ckpt-128\n",
      "batch 128: loss 0.461461\n",
      "model saved to ./save\\ckpt-129\n",
      "batch 129: loss 0.396944\n",
      "model saved to ./save\\ckpt-130\n",
      "batch 130: loss 0.209582\n",
      "model saved to ./save\\ckpt-131\n",
      "batch 131: loss 0.547881\n",
      "model saved to ./save\\ckpt-132\n",
      "batch 132: loss 0.385067\n",
      "model saved to ./save\\ckpt-133\n",
      "batch 133: loss 0.214929\n",
      "model saved to ./save\\ckpt-134\n",
      "batch 134: loss 0.275867\n",
      "model saved to ./save\\ckpt-135\n",
      "batch 135: loss 0.380952\n",
      "model saved to ./save\\ckpt-136\n",
      "batch 136: loss 0.398702\n",
      "model saved to ./save\\ckpt-137\n",
      "batch 137: loss 0.296812\n",
      "model saved to ./save\\ckpt-138\n",
      "batch 138: loss 0.285711\n",
      "model saved to ./save\\ckpt-139\n",
      "batch 139: loss 0.403538\n",
      "model saved to ./save\\ckpt-140\n",
      "batch 140: loss 0.433744\n",
      "model saved to ./save\\ckpt-141\n",
      "batch 141: loss 0.468563\n",
      "model saved to ./save\\ckpt-142\n",
      "batch 142: loss 0.338503\n",
      "model saved to ./save\\ckpt-143\n",
      "batch 143: loss 0.602734\n",
      "model saved to ./save\\ckpt-144\n",
      "batch 144: loss 0.314286\n",
      "model saved to ./save\\ckpt-145\n",
      "batch 145: loss 0.452367\n",
      "model saved to ./save\\ckpt-146\n",
      "batch 146: loss 0.847662\n",
      "model saved to ./save\\ckpt-147\n",
      "batch 147: loss 0.397210\n",
      "model saved to ./save\\ckpt-148\n",
      "batch 148: loss 0.333782\n",
      "model saved to ./save\\ckpt-149\n",
      "batch 149: loss 0.356208\n",
      "model saved to ./save\\ckpt-150\n",
      "batch 150: loss 0.364728\n",
      "model saved to ./save\\ckpt-151\n",
      "batch 151: loss 0.281225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to ./save\\ckpt-152\n",
      "batch 152: loss 0.203906\n",
      "model saved to ./save\\ckpt-153\n",
      "batch 153: loss 0.353706\n",
      "model saved to ./save\\ckpt-154\n",
      "batch 154: loss 0.341884\n",
      "model saved to ./save\\ckpt-155\n",
      "batch 155: loss 0.313355\n",
      "model saved to ./save\\ckpt-156\n",
      "batch 156: loss 0.313145\n",
      "model saved to ./save\\ckpt-157\n",
      "batch 157: loss 0.190414\n",
      "model saved to ./save\\ckpt-158\n",
      "batch 158: loss 0.333955\n",
      "model saved to ./save\\ckpt-159\n",
      "batch 159: loss 0.342028\n",
      "model saved to ./save\\ckpt-160\n",
      "batch 160: loss 0.275797\n",
      "model saved to ./save\\ckpt-161\n",
      "batch 161: loss 0.247955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e048ed9af094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model saved to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, checkpoint_number, check_interval)\u001b[0m\n\u001b[0;32m    817\u001b[0m     \u001b[1;31m# a preemption while deleting will be more likely to see the new checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;31m# this way.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sweep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;31m# Write out the Checkpoint proto a second time, now without the deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\u001b[0m in \u001b[0;36m_record_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mall_model_checkpoint_timestamps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mlast_preserved_timestamp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_preserved_timestamp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         save_relative_paths=True)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\u001b[0m in \u001b[0;36mupdate_checkpoint_state_internal\u001b[1;34m(save_dir, model_checkpoint_path, all_model_checkpoint_paths, latest_filename, save_relative_paths, all_model_checkpoint_timestamps, last_preserved_timestamp)\u001b[0m\n\u001b[0;32m    246\u001b[0m   \u001b[1;31m# file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m   file_io.atomic_write_string_to_file(coord_checkpoint_filename,\n\u001b[1;32m--> 248\u001b[1;33m                                       text_format.MessageToString(ckpt))\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[1;34m(filename, contents, overwrite)\u001b[0m\n\u001b[0;32m    566\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[0mtemp_pathname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".tmp\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mwrite_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m       \u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[1;34m(filename, file_content)\u001b[0m\n\u001b[0;32m    332\u001b[0m   \"\"\"\n\u001b[0;32m    333\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\envs\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                                            \"File isn't open for writing\")\n\u001b[0;32m     86\u001b[0m       self._writable_file = _pywrap_file_io.WritableFile(\n\u001b[1;32m---> 87\u001b[1;33m           compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_prepare_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "# 使用tf.train.CheckpointManager来管理CheckPoint\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory='./save', max_to_keep=3)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    #使用checkpoint把model.variables存储起来\n",
    "    path = manager.save()\n",
    "    print(\"model saved to %s\" % path)\n",
    "    \n",
    "# loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "\n",
    "loss = tf.keras.losses.categorical_crossentropy(\n",
    "    y_true=tf.one_hot(y, depth=tf.shape(y_pred)[-1]),\n",
    "    y_pred=y_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.899800\n"
     ]
    }
   ],
   "source": [
    "# 从checkpoint中加载回之前的model.variables\n",
    "model = MLP()\n",
    "checkpoint = tf.train.Checkpoint(myAwesomeModel=model)      \n",
    "checkpoint.restore(tf.train.latest_checkpoint('./save'))\n",
    "\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
